"""
ULEPSZONY SYSTEM CZYSZCZENIA LOGÃ“W
==================================
RozrÃ³Å¼nia dane sesyjne (do czyszczenia) od archiwalnych (do zachowania)
"""

from pathlib import Path
import shutil
import json
from datetime import datetime, timedelta
from typing import List, Dict
import re

class SmartLogCleaner:
    """Inteligentne czyszczenie z rozrÃ³Å¼nieniem na sesyjne vs archiwalne"""
    
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.logs_dir = self.project_root / "logs"
        self.today = datetime.now().strftime("%Y%m%d")
    
    def clean_session_only(self) -> Dict[str, int]:
        """CzyÅ›Ä‡ TYLKO bieÅ¼Ä…cÄ… sesjÄ™ - zachowaj dane ML i archiwa"""
        stats = {
            'session_files': 0,
            'strategic_orders': 0, 
            'purchased_tokens': 0,
            'start_tokens': 0,
            'preserved_ml': 0
        }
        
        print("ðŸ§¹ CZYSZCZENIE SESYJNE (zachowujÄ™ ML i archiwa)")
        print("-" * 50)
        
        # 1. Rozkazy strategiczne
        orders_file = self.project_root / "data" / "strategic_orders.json"
        if orders_file.exists():
            orders_file.unlink()
            stats['strategic_orders'] = 1
            print("âœ… UsuniÄ™to rozkazy strategiczne")
        
        # 2. Zakupione Å¼etony
        stats['purchased_tokens'] = self._clean_purchased_tokens()
        
        # 3. SESYJNE CZYSZCZENIE: ZACHOWUJEMY start_tokens.json!
        # KRYTYCZNE: start_tokens.json NIGDY nie jest czyszczony - zawiera rozmieszczenie Å¼etonÃ³w na mapie!
        start_tokens_file = self.project_root / "assets" / "start_tokens.json"
        if start_tokens_file.exists():
            # SprawdÅº czy plik ma sensowny rozmiar (powyÅ¼ej 1000 bajtÃ³w = prawdziwe dane)
            size = start_tokens_file.stat().st_size
            if size > 1000:
                print(f"ðŸŽ¯ start_tokens.json CHRONIONY (rozmiar: {size} bajtÃ³w)")
            else:
                print(f"âš ï¸  start_tokens.json podejrzanie maÅ‚y ({size} bajtÃ³w) - prawdopodobnie testowy")
        else:
            print("âš ï¸  start_tokens.json BRAK - to moÅ¼e byÄ‡ problem!")
        stats['start_tokens'] = 0  # ZAWSZE 0 - nigdy nie czyÅ›cimy tego pliku przy sesji
        
        # 4. BieÅ¼Ä…ce logi sesyjne - WSZYSTKIE PRAWDZIWE PLIKI
        if self.logs_dir.exists():
            # NAPRAWIONE: CzyÅ›ci wszystkie pliki CSV/LOG z dzisiejszÄ… datÄ…
            session_files_patterns = [
                f"*{self.today}*",           # Wzorzec daty: 20250915
                f"*{self.today[:4]}-{self.today[4:6]}-{self.today[6:8]}*"  # Wzorzec: 2025-09-15
            ]
            
            # Foldery do zachowania (dane ML)
            preserved_paths = [
                self.logs_dir / "dane_ml",
                self.logs_dir / "analysis" / "ml_ready",
                self.logs_dir / "analysis" / "raporty", 
                self.logs_dir / "analysis" / "statystyki",
                self.logs_dir / "archiwum_sesji"
            ]
            
            for pattern in session_files_patterns:
                for log_file in self.logs_dir.rglob(pattern):
                    if log_file.is_file() and log_file.suffix in ['.csv', '.log', '.txt']:
                        # SprawdÅº czy to nie jest w folderze do zachowania
                        should_preserve = any(
                            preserved_path in log_file.parents or preserved_path == log_file.parent
                            for preserved_path in preserved_paths
                        )
                        
                        if should_preserve:
                            stats['preserved_ml'] += 1
                            print(f"ðŸ’¾ Zachowano ML: {log_file.relative_to(self.logs_dir)}")
                        else:
                            log_file.unlink()
                            stats['session_files'] += 1
                            print(f"âœ… UsuniÄ™to: {log_file.relative_to(self.logs_dir)}")
            
            # Dodatkowo usuÅ„ puste foldery current_session i sesja_aktualna
            for session_folder in ['current_session', 'sesja_aktualna']:
                session_path = self.logs_dir / session_folder
                if session_path.exists():
                    # UsuÅ„ puste podfoldery
                    for subfolder in session_path.rglob('*'):
                        if subfolder.is_dir() and not any(subfolder.iterdir()):
                            subfolder.rmdir()
                            print(f"ðŸ—‘ï¸ Pusty folder: {subfolder.relative_to(self.logs_dir)}")
        
        print("-" * 50)
        print(f"âœ… SESJA WYCZYSZCZONA:")
        print(f"   ðŸ“„ PlikÃ³w sesyjnych: {stats['session_files']}")  
        print(f"   ðŸ’¾ Zachowanych ML: {stats['preserved_ml']}")
        print(f"   ðŸŽ¯ Rozkazy: {stats['strategic_orders']}")
        print(f"   ðŸª™ Å»etony: {stats['purchased_tokens']}")
        
        return stats
    
    def clean_preserve_ml(self) -> Dict[str, int]:
        """PeÅ‚ne czyszczenie z zachowaniem danych ML"""
        stats = self.clean_session_only()
        
        print("\nðŸ§¹ CZYSZCZENIE PEÅNE (zachowujÄ™ dane ML)")
        print("-" * 50)
        
        # Dodatkowo usuÅ„ WSZYSTKIE stare logi (nie tylko z dzisiaj)
        old_files = 0
        if self.logs_dir.exists():
            preserved_paths = [
                self.logs_dir / "analysis" / "ml_ready",
            ]
            
            # UsuÅ„ wszystkie dane_*.csv OPRÃ“CZ ml_ready
            for log_file in self.logs_dir.rglob("dane_*.csv"):
                should_preserve = any(
                    preserved_path in log_file.parents 
                    for preserved_path in preserved_paths
                )
                
                if not should_preserve and self.today not in log_file.name:
                    log_file.unlink()
                    old_files += 1
                    print(f"ðŸ—‘ï¸ Stary log: {log_file.relative_to(self.logs_dir)}")
            
            # UsuÅ„ wszystkie python_*.log
            for log_file in self.logs_dir.rglob("python_*.log"):
                log_file.unlink()
                old_files += 1
        
        stats['old_files'] = old_files
        print(f"âœ… PEÅNE CZYSZCZENIE: +{old_files} starych plikÃ³w")
        
        return stats
    
    def archive_session(self, archive_days: int = 30) -> Dict[str, int]:
        """Archiwizuj bieÅ¼Ä…cÄ… sesjÄ™ do archive/YYYY-MM-DD/"""
        archive_dir = self.project_root / "archive" / self.today
        archive_dir.mkdir(parents=True, exist_ok=True)
        
        stats = {'archived_files': 0, 'ml_datasets': 0}
        
        print(f"ðŸ“š ARCHIWIZACJA SESJI â†’ archive/{self.today}/")
        print("-" * 50)
        
        if self.logs_dir.exists():
            # Archiwizuj pliki z dzisiaj
            for log_file in self.logs_dir.rglob(f"*{self.today}*"):
                if log_file.is_file() and log_file.suffix in ['.csv', '.json', '.log']:
                    # Zachowaj strukturÄ™ folderÃ³w
                    rel_path = log_file.relative_to(self.logs_dir)
                    archive_path = archive_dir / rel_path
                    archive_path.parent.mkdir(parents=True, exist_ok=True)
                    
                    shutil.copy2(log_file, archive_path)
                    stats['archived_files'] += 1
                    
                    if 'ml_ready' in str(log_file):
                        stats['ml_datasets'] += 1
                    
                    print(f"ðŸ“¦ {rel_path}")
        
        print(f"âœ… ZARCHIWIZOWANO: {stats['archived_files']} plikÃ³w")
        print(f"   ðŸ“Š w tym ML datasets: {stats['ml_datasets']}")
        
        # Po archiwizacji - wyczyÅ›Ä‡ sesjÄ™
        clean_stats = self.clean_session_only()
        stats.update(clean_stats)
        
        return stats
    
    def clean_old_archives(self, days_old: int = 30) -> Dict[str, int]:
        """UsuÅ„ archiwa starsze niÅ¼ N dni"""
        archive_root = self.project_root / "archive"
        if not archive_root.exists():
            return {'removed_archives': 0}
        
        cutoff_date = datetime.now() - timedelta(days=days_old)
        removed_count = 0
        
        print(f"ðŸ—‚ï¸ CZYSZCZENIE STARYCH ARCHIWÃ“W (>{days_old} dni)")
        print("-" * 50)
        
        for archive_dir in archive_root.iterdir():
            if archive_dir.is_dir():
                # SprawdÅº czy nazwa to data YYYYMMDD
                match = re.match(r'(\\d{8})', archive_dir.name)
                if match:
                    try:
                        archive_date = datetime.strptime(match.group(1), '%Y%m%d')
                        if archive_date < cutoff_date:
                            shutil.rmtree(archive_dir)
                            removed_count += 1
                            print(f"ðŸ—‘ï¸ UsuniÄ™to archiwum: {archive_dir.name}")
                    except ValueError:
                        continue
        
        print(f"âœ… UsuniÄ™to {removed_count} starych archiwÃ³w")
        return {'removed_archives': removed_count}
    
    def get_ml_stats(self) -> Dict[str, any]:
        """PokaÅ¼ statystyki danych ML"""
        ml_dir = self.logs_dir / "analysis" / "ml_ready"
        if not ml_dir.exists():
            return {'status': 'brak_danych_ml'}
        
        stats = {
            'csv_files': 0,
            'meta_files': 0, 
            'total_size_kb': 0,
            'datasets': []
        }
        
        for file in ml_dir.glob("*.csv"):
            stats['csv_files'] += 1
            stats['total_size_kb'] += file.stat().st_size / 1024
            
            # Szukaj odpowiadajÄ…cego pliku meta
            meta_file = file.with_name(f"{file.stem}_meta.json")
            if meta_file.exists():
                stats['meta_files'] += 1
                try:
                    meta_data = json.loads(meta_file.read_text(encoding='utf-8'))
                    stats['datasets'].append({
                        'name': file.stem,
                        'records': meta_data.get('rozmiar_datasetu', 0),
                        'features': meta_data.get('liczba_cech', 0),
                        'size_kb': file.stat().st_size / 1024
                    })
                except:
                    pass
        
        return stats
    
    def _clean_purchased_tokens(self) -> int:
        """Pomocnicza - czyÅ›Ä‡ zakupione Å¼etony"""
        tokens_dir = self.project_root / "assets" / "tokens"
        deleted = 0
        
        if tokens_dir.exists():
            # UsuÅ„ foldery nowe_dla_*
            for folder in tokens_dir.glob("nowe_dla_*"):
                if folder.is_dir():
                    shutil.rmtree(folder)
                    deleted += 1
                    print(f"âœ… Folder Å¼etonÃ³w: {folder.name}")
            
            # UsuÅ„ pliki nowy_* z aktualne/
            aktualne_dir = tokens_dir / "aktualne"
            if aktualne_dir.exists():
                for file in aktualne_dir.glob("nowy_*"):
                    file.unlink()
                    deleted += 1
                    print(f"âœ… Å»eton: {file.name}")
        
        # CzyÅ›Ä‡ z index.json
        index_path = self.project_root / "assets" / "tokens" / "index.json"
        if index_path.exists():
            try:
                index_data = json.loads(index_path.read_text(encoding='utf-8'))
                cleaned = [t for t in index_data if not t.get("id", "").startswith("nowy_")]
                if len(cleaned) < len(index_data):
                    index_path.write_text(json.dumps(cleaned, indent=2, ensure_ascii=False), encoding='utf-8')
                    print(f"âœ… Index: usuniÄ™to {len(index_data) - len(cleaned)} Å¼etonÃ³w")
            except:
                pass
        
        return deleted


# Funkcje dla kompatybilnoÅ›ci z istniejÄ…cym systemem
def smart_clean_session():
    """Czyszczenie sesyjne - zachowaj ML"""
    cleaner = SmartLogCleaner()
    return cleaner.clean_session_only()

def smart_clean_full():
    """PeÅ‚ne czyszczenie - zachowaj ML"""  
    cleaner = SmartLogCleaner()
    return cleaner.clean_preserve_ml()

def smart_archive_and_clean():
    """Archiwizuj i wyczyÅ›Ä‡"""
    cleaner = SmartLogCleaner()
    return cleaner.archive_session()

def show_ml_status():
    """PokaÅ¼ status danych ML"""
    cleaner = SmartLogCleaner()
    stats = cleaner.get_ml_stats()
    
    print("ðŸ“Š STATUS DANYCH ML")
    print("-" * 30)
    if stats.get('status') == 'brak_danych_ml':
        print("âŒ Brak danych ML")
        return
    
    print(f"ðŸ“„ PlikÃ³w CSV: {stats['csv_files']}")
    print(f"ðŸ“‹ PlikÃ³w meta: {stats['meta_files']}")
    print(f"ðŸ’¾ Rozmiar: {stats['total_size_kb']:.1f} KB")
    print("ðŸ“Š Datasety:")
    
    for ds in stats['datasets']:
        print(f"  â€¢ {ds['name']}: {ds['records']} rekordÃ³w, {ds['features']} cech ({ds['size_kb']:.1f} KB)")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Inteligentne czyszczenie logÃ³w')
    parser.add_argument('--mode', choices=['session', 'full', 'archive', 'old_archives', 'ml_status'], 
                       default='session')
    parser.add_argument('--days', type=int, default=30, help='Dni dla old_archives')
    
    args = parser.parse_args()
    
    cleaner = SmartLogCleaner()
    
    if args.mode == 'session':
        cleaner.clean_session_only()
    elif args.mode == 'full':
        cleaner.clean_preserve_ml()
    elif args.mode == 'archive':
        cleaner.archive_session()
    elif args.mode == 'old_archives':
        cleaner.clean_old_archives(args.days)
    elif args.mode == 'ml_status':
        show_ml_status()